{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672222d8",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install jiwer einops addict easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a886bf",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e57e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10781b91",
   "metadata": {},
   "source": [
    "## 3. Download and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d717521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e06b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import os\n",
    "\n",
    "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"./deepseek_ocr\",\n",
    "    load_in_4bit = False,\n",
    "    auto_model = AutoModel,\n",
    "    trust_remote_code=True,\n",
    "    unsloth_force_compile=True,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cca487",
   "metadata": {},
   "source": [
    "## 4. Download and Extract Dataset\n",
    "\n",
    "**Important:** Update `ZIP_PATH` to match your Google Drive zip file location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚ö†Ô∏è UPDATE THIS PATH TO YOUR ZIP FILE ON GOOGLE DRIVE\n",
    "ZIP_PATH = '/content/drive/MyDrive/UIT_HWDB_word.zip'\n",
    "EXTRACT_DIR = '/content/UIT_HWDB_word'\n",
    "\n",
    "print(f\"üì¶ Extracting dataset from: {ZIP_PATH}\")\n",
    "print(f\"üìÇ Extracting to: {EXTRACT_DIR}\")\n",
    "\n",
    "# Extract zip file\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/')\n",
    "\n",
    "print(f\"‚úÖ Extraction complete!\")\n",
    "\n",
    "# Set data directories\n",
    "TRAIN_DIR = os.path.join(EXTRACT_DIR, 'train_data')\n",
    "TEST_DIR = os.path.join(EXTRACT_DIR, 'test_data')\n",
    "\n",
    "print(f\"\\n‚úì Train directory exists: {os.path.exists(TRAIN_DIR)}\")\n",
    "print(f\"‚úì Test directory exists: {os.path.exists(TEST_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdbfd8",
   "metadata": {},
   "source": [
    "## 5. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_folders(base_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from folder structure with label.json files.\n",
    "    Converts to Deepseek OCR conversation format with PIL Image objects.\n",
    "    \n",
    "    Structure:\n",
    "    base_dir/\n",
    "        1/label.json + images\n",
    "        2/label.json + images\n",
    "        ...\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    folders = sorted(\n",
    "        [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))],\n",
    "        key=lambda x: int(x)\n",
    "    )\n",
    "    \n",
    "    instruction = \"<image>\\nFree OCR. \"\n",
    "    \n",
    "    for folder in tqdm(folders, desc=f\"Loading {os.path.basename(base_dir)}\"):\n",
    "        label_path = os.path.join(base_dir, folder, 'label.json')\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Load labels\n",
    "        with open(label_path, 'r', encoding='utf-8') as f:\n",
    "            labels = json.load(f)\n",
    "        \n",
    "        # Process each image\n",
    "        for img_name, label in labels.items():\n",
    "            img_path = os.path.join(base_dir, folder, img_name)\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load image as PIL object (required by Deepseek OCR)\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                # Create conversation format\n",
    "                conversation = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"<|User|>\",\n",
    "                            \"content\": instruction,\n",
    "                            \"images\": [image]  # PIL Image object\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"<|Assistant|>\",\n",
    "                            \"content\": label.strip()\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                conversations.append(conversation)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nüì• Loading training data...\")\n",
    "train_dataset = load_dataset_from_folders(TRAIN_DIR)\n",
    "print(f\"‚úÖ Loaded {len(train_dataset):,} training samples\")\n",
    "\n",
    "print(\"\\nüì• Loading test data...\")\n",
    "test_dataset = load_dataset_from_folders(TEST_DIR)\n",
    "print(f\"‚úÖ Loaded {len(test_dataset):,} test samples\")\n",
    "\n",
    "print(f\"\\nüìä Total: {len(train_dataset) + len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data format\n",
    "print(\"üîç Verifying data structure:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  ‚úì Messages: {len(sample['messages'])}\")\n",
    "print(f\"  ‚úì User content: {sample['messages'][0]['content']}\")\n",
    "print(f\"  ‚úì Images: {len(sample['messages'][0]['images'])} PIL Image(s)\")\n",
    "print(f\"  ‚úì Image type: {type(sample['messages'][0]['images'][0])}\")\n",
    "print(f\"  ‚úì Image size: {sample['messages'][0]['images'][0].size}\")\n",
    "print(f\"  ‚úì Assistant content: '{sample['messages'][1]['content']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981dbe99",
   "metadata": {},
   "source": [
    "## 6. (Optional) Test Baseline Model\n",
    "\n",
    "Test the pretrained model before finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test sample\n",
    "test_sample = test_dataset[50]\n",
    "test_image = test_sample['messages'][0]['images'][0]\n",
    "ground_truth = test_sample['messages'][1]['content']\n",
    "\n",
    "# Save and display\n",
    "test_image.save(\"test_image.jpg\")\n",
    "from IPython.display import display\n",
    "print(f\"Ground truth: '{ground_truth}'\")\n",
    "display(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline inference\n",
    "prompt = \"<image>\\nFree OCR. \"\n",
    "print(\"Running baseline model...\")\n",
    "res = model.infer(\n",
    "    tokenizer, \n",
    "    prompt=prompt, \n",
    "    image_file='test_image.jpg', \n",
    "    output_path='./output', \n",
    "    base_size=1024, \n",
    "    image_size=640, \n",
    "    crop_mode=True, \n",
    "    save_results=False,\n",
    "    test_compress=False\n",
    ")\n",
    "print(f\"\\nüìù Baseline: '{res}'\")\n",
    "print(f\"üìù Ground truth: '{ground_truth}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e4d0c",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Add LoRA Adapters\n",
    "\n",
    "Parameter-efficient finetuning with LoRA - only train ~1% of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4912d4",
   "metadata": {},
   "source": [
    "## 8. Prepare Training Dataset\n",
    "\n",
    "Choose dataset size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e695cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Full dataset (107K+ samples, takes longer)\n",
    "converted_dataset = train_dataset\n",
    "\n",
    "# Option 2: Subset for faster experimentation (uncomment below)\n",
    "# converted_dataset = train_dataset[:5000]\n",
    "\n",
    "print(f\"üéØ Using {len(converted_dataset):,} samples for training\")\n",
    "\n",
    "# Verify format\n",
    "sample = converted_dataset[0]\n",
    "print(f\"\\n‚úì Sample check:\")\n",
    "print(f\"  Role: {sample['messages'][0]['role']}\")\n",
    "print(f\"  Content: {sample['messages'][0]['content']}\")\n",
    "print(f\"  Images: {type(sample['messages'][0]['images'][0])}\")\n",
    "print(f\"  Label: '{sample['messages'][1]['content']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6321181",
   "metadata": {},
   "source": [
    "## 9. Create Data Collator\n",
    "\n",
    "This handles image preprocessing and batching for Deepseek OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title DeepSeekOCRDataCollator\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from PIL import Image, ImageOps\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import io\n",
    "\n",
    "from deepseek_ocr.modeling_deepseekocr import (\n",
    "    text_encode,\n",
    "    BasicImageTransform,\n",
    "    dynamic_preprocess,\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class DeepSeekOCRDataCollator:\n",
    "    tokenizer: Any\n",
    "    model: Any\n",
    "    image_size: int = 640\n",
    "    base_size: int = 1024\n",
    "    crop_mode: bool = True\n",
    "    image_token_id: int = 128815\n",
    "    train_on_responses_only: bool = True\n",
    "\n",
    "    def __init__(self, tokenizer, model, image_size=640, base_size=1024, \n",
    "                 crop_mode=True, train_on_responses_only=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.image_size = image_size\n",
    "        self.base_size = base_size\n",
    "        self.crop_mode = crop_mode\n",
    "        self.image_token_id = 128815\n",
    "        self.dtype = model.dtype\n",
    "        self.train_on_responses_only = train_on_responses_only\n",
    "        self.image_transform = BasicImageTransform(\n",
    "            mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), normalize=True\n",
    "        )\n",
    "        self.patch_size = 16\n",
    "        self.downsample_ratio = 4\n",
    "        self.bos_id = tokenizer.bos_token_id if hasattr(tokenizer, 'bos_token_id') else 0\n",
    "\n",
    "    def deserialize_image(self, image_data) -> Image.Image:\n",
    "        if isinstance(image_data, Image.Image):\n",
    "            return image_data.convert(\"RGB\")\n",
    "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
    "            return Image.open(io.BytesIO(image_data['bytes'])).convert(\"RGB\")\n",
    "        raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n",
    "\n",
    "    def process_image(self, image: Image.Image):\n",
    "        images_list, images_crop_list, images_spatial_crop = [], [], []\n",
    "        \n",
    "        if self.crop_mode:\n",
    "            if image.size[0] <= 640 and image.size[1] <= 640:\n",
    "                crop_ratio = (1, 1)\n",
    "                images_crop_raw = []\n",
    "            else:\n",
    "                images_crop_raw, crop_ratio = dynamic_preprocess(\n",
    "                    image, min_num=2, max_num=9, image_size=self.image_size, use_thumbnail=False\n",
    "                )\n",
    "            \n",
    "            global_view = ImageOps.pad(\n",
    "                image, (self.base_size, self.base_size),\n",
    "                color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
    "            )\n",
    "            images_list.append(self.image_transform(global_view).to(self.dtype))\n",
    "            width_crop_num, height_crop_num = crop_ratio\n",
    "            images_spatial_crop.append([width_crop_num, height_crop_num])\n",
    "            \n",
    "            if width_crop_num > 1 or height_crop_num > 1:\n",
    "                for crop_img in images_crop_raw:\n",
    "                    images_crop_list.append(self.image_transform(crop_img).to(self.dtype))\n",
    "            \n",
    "            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
    "            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
    "            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base + [self.image_token_id]\n",
    "            \n",
    "            if width_crop_num > 1 or height_crop_num > 1:\n",
    "                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (num_queries * height_crop_num)\n",
    "        else:\n",
    "            crop_ratio = (1, 1)\n",
    "            images_spatial_crop.append([1, 1])\n",
    "            if self.base_size <= 640:\n",
    "                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n",
    "                images_list.append(self.image_transform(resized_image).to(self.dtype))\n",
    "            else:\n",
    "                global_view = ImageOps.pad(\n",
    "                    image, (self.base_size, self.base_size),\n",
    "                    color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
    "                )\n",
    "                images_list.append(self.image_transform(global_view).to(self.dtype))\n",
    "            \n",
    "            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
    "            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries + [self.image_token_id]\n",
    "        \n",
    "        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n",
    "\n",
    "    def process_single_sample(self, messages: List[Dict]):\n",
    "        images = []\n",
    "        for message in messages:\n",
    "            if \"images\" in message and message[\"images\"]:\n",
    "                for img_data in message[\"images\"]:\n",
    "                    if img_data is not None:\n",
    "                        images.append(self.deserialize_image(img_data))\n",
    "        \n",
    "        if not images:\n",
    "            raise ValueError(\"No images found in sample\")\n",
    "        \n",
    "        tokenized_str = [self.bos_id]\n",
    "        images_seq_mask = [False]\n",
    "        images_list, images_crop_list, images_spatial_crop = [], [], []\n",
    "        prompt_token_count = -1\n",
    "        assistant_started = False\n",
    "        image_idx = 0\n",
    "        \n",
    "        for message in messages:\n",
    "            role, content = message[\"role\"], message[\"content\"]\n",
    "            \n",
    "            if role == \"<|Assistant|>\":\n",
    "                if not assistant_started:\n",
    "                    prompt_token_count = len(tokenized_str)\n",
    "                    assistant_started = True\n",
    "                content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n",
    "            \n",
    "            text_splits = content.split('<image>')\n",
    "            for i, text_sep in enumerate(text_splits):\n",
    "                tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n",
    "                tokenized_str.extend(tokenized_sep)\n",
    "                images_seq_mask.extend([False] * len(tokenized_sep))\n",
    "                \n",
    "                if i < len(text_splits) - 1:\n",
    "                    if image_idx >= len(images):\n",
    "                        raise ValueError(\"Mismatch between <image> tokens and actual images\")\n",
    "                    img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(images[image_idx])\n",
    "                    images_list.extend(img_list)\n",
    "                    images_crop_list.extend(crop_list)\n",
    "                    images_spatial_crop.extend(spatial_crop)\n",
    "                    tokenized_str.extend(tok_img)\n",
    "                    images_seq_mask.extend([True] * len(tok_img))\n",
    "                    image_idx += 1\n",
    "        \n",
    "        if not assistant_started:\n",
    "            prompt_token_count = len(tokenized_str)\n",
    "        \n",
    "        images_ori = torch.stack(images_list, dim=0)\n",
    "        images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n",
    "        images_crop = torch.stack(images_crop_list) if images_crop_list else torch.zeros((1, 3, self.base_size, self.base_size), dtype=self.dtype)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n",
    "            \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n",
    "            \"images_ori\": images_ori,\n",
    "            \"images_crop\": images_crop,\n",
    "            \"images_spatial_crop\": images_spatial_crop_tensor,\n",
    "            \"prompt_token_count\": prompt_token_count,\n",
    "        }\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]):\n",
    "        batch_data = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                batch_data.append(self.process_single_sample(feature['messages']))\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not batch_data:\n",
    "            raise ValueError(\"No valid samples in batch\")\n",
    "        \n",
    "        input_ids = pad_sequence([item['input_ids'] for item in batch_data], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        images_seq_mask = pad_sequence([item['images_seq_mask'] for item in batch_data], batch_first=True, padding_value=False)\n",
    "        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        labels[images_seq_mask] = -100\n",
    "        \n",
    "        if self.train_on_responses_only:\n",
    "            for idx, prompt_count in enumerate(prompt_token_counts):\n",
    "                if prompt_count > 0:\n",
    "                    labels[idx, :prompt_count] = -100\n",
    "        \n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        images_batch = [(item['images_crop'], item['images_ori']) for item in batch_data]\n",
    "        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim=0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"images\": images_batch,\n",
    "            \"images_seq_mask\": images_seq_mask,\n",
    "            \"images_spatial_crop\": images_spatial_crop,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b36b814",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "Configure training parameters and start finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from unsloth import is_bf16_supported\n",
    "\n",
    "FastVisionModel.for_training(model)\n",
    "\n",
    "data_collator = DeepSeekOCRDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    image_size=640,\n",
    "    base_size=1024,\n",
    "    crop_mode=True,\n",
    "    train_on_responses_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=converted_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,  # For quick test. Set num_train_epochs=1 and max_steps=None for full run\n",
    "        # num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        fp16=not is_bf16_supported(),\n",
    "        bf16=is_bf16_supported(),\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,  # Required for vision models\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}, Max memory: {max_memory} GB\")\n",
    "print(f\"Memory reserved: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ee2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"‚è±Ô∏è  Training time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "print(f\"üíæ Peak memory: {used_memory} GB ({used_percentage}%)\")\n",
    "print(f\"üíæ Memory for LoRA: {used_memory_for_lora} GB ({lora_percentage}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f5d3a",
   "metadata": {},
   "source": [
    "## 11. Test Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on same sample as baseline\n",
    "print(\"Testing finetuned model...\")\n",
    "res = model.infer(\n",
    "    tokenizer, \n",
    "    prompt=\"<image>\\nFree OCR. \", \n",
    "    image_file='test_image.jpg',\n",
    "    output_path='./output',\n",
    "    image_size=640,\n",
    "    base_size=1024,\n",
    "    crop_mode=True,\n",
    "    save_results=False\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Finetuned: '{res}'\")\n",
    "print(f\"üìù Ground truth: '{ground_truth}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb9659",
   "metadata": {},
   "source": [
    "## 12. Save Model\n",
    "\n",
    "Save LoRA adapters for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "SAVE_PATH = \"/content/drive/MyDrive/deepseek_ocr_lora\"\n",
    "\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec7153",
   "metadata": {},
   "source": [
    "## 13. (Optional) Load Saved Model\n",
    "\n",
    "Load the finetuned model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load saved model\n",
    "\"\"\"\n",
    "from unsloth import FastVisionModel\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=SAVE_PATH,\n",
    "    load_in_4bit=False,\n",
    "    auto_model=AutoModel,\n",
    "    trust_remote_code=True,\n",
    "    unsloth_force_compile=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# Run inference\n",
    "res = model.infer(\n",
    "    tokenizer, \n",
    "    prompt=\"<image>\\nFree OCR. \", \n",
    "    image_file='test_image.jpg',\n",
    "    output_path='./output',\n",
    "    image_size=640,\n",
    "    base_size=1024,\n",
    "    crop_mode=True\n",
    ")\n",
    "print(f\"Result: {res}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3eca8",
   "metadata": {},
   "source": [
    "## 14. (Optional) Save as 16-bit Merged Model\n",
    "\n",
    "Save full model in float16 for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7130bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save merged model\n",
    "\"\"\"\n",
    "# Save locally\n",
    "model.save_pretrained_merged(\"unsloth_finetune_merged\", tokenizer)\n",
    "\n",
    "# Or push to Hugging Face Hub\n",
    "# model.push_to_hub_merged(\"YOUR_USERNAME/deepseek-ocr-vietnamese\", tokenizer, token=\"YOUR_TOKEN\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c783e95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done! üéâ\n",
    "\n",
    "You've successfully finetuned Deepseek OCR on Vietnamese handwriting data.\n",
    "\n",
    "**Next steps:**\n",
    "- Evaluate on full test set\n",
    "- Calculate metrics (CER, WER)\n",
    "- Deploy for inference\n",
    "- Try different hyperparameters\n",
    "\n",
    "**Resources:**\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [Discord Community](https://discord.gg/unsloth)\n",
    "- [GitHub](https://github.com/unslothai/unsloth)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
