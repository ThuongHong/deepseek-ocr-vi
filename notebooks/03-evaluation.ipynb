{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257d5d7a",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install jiwer einops addict easydict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b26cc0",
   "metadata": {},
   "source": [
    "## 2. Environment Setup (Colab / Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b757ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive only for Colab\n",
    "IS_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "\n",
    "if not IS_KAGGLE:\n",
    "    print(\"\\nüìÅ Mounting Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted!\")\n",
    "else:\n",
    "    print(\"\\nüìÅ Using Kaggle environment settings\")\n",
    "    print(\"‚úÖ Kaggle input/output paths will be used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b705d",
   "metadata": {},
   "source": [
    "## 3. Choose Model to Evaluate\n",
    "\n",
    "Select which model to evaluate:\n",
    "- **Baseline**: Pretrained DeepSeek OCR (before finetuning)\n",
    "- **Finetuned**: LoRA finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è SELECT MODEL TO EVALUATE\n",
    "# Set to 'baseline' or 'finetuned'\n",
    "MODEL_TYPE = 'finetuned'  # Change this to 'baseline' to evaluate pretrained model\n",
    "\n",
    "print(f\"\\nüéØ Selected model: {MODEL_TYPE.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe377bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
    "\n",
    "if MODEL_TYPE == 'baseline':\n",
    "    print(\"\\nüì¶ Loading baseline (pretrained) model...\")\n",
    "    # Download baseline model\n",
    "    snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir=\"deepseek_ocr\")\n",
    "    MODEL_PATH = \"./deepseek_ocr\"\n",
    "    print(f\"‚úÖ Downloaded baseline model to: {MODEL_PATH}\")\n",
    "    \n",
    "else:  # finetuned\n",
    "    print(\"\\nüì¶ Loading finetuned model...\")\n",
    "    # Setup model path based on environment\n",
    "    if not IS_KAGGLE:\n",
    "        # ‚ö†Ô∏è UPDATE THIS PATH TO YOUR FINETUNED MODEL ON GOOGLE DRIVE\n",
    "        MODEL_PATH = '/content/drive/MyDrive/deepseek_ocr_lora'\n",
    "        print(f\"üì¶ Colab: Loading model from: {MODEL_PATH}\")\n",
    "    else:\n",
    "        # ‚ö†Ô∏è UPDATE 'your-model-folder' to match your Kaggle input\n",
    "        MODEL_PATH = '/kaggle/input/deepseek-ocr-lora'  # Change this to your model path\n",
    "        print(f\"üì¶ Kaggle: Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    load_in_4bit=False,\n",
    "    auto_model=AutoModel,\n",
    "    trust_remote_code=True,\n",
    "    unsloth_force_compile=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"‚úì Evaluating: {MODEL_TYPE.upper()} model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19e051",
   "metadata": {},
   "source": [
    "## 4. Download and Extract Test Dataset\n",
    "\n",
    "**Colab:** Update `ZIP_PATH` to your Google Drive location\n",
    "\n",
    "**Kaggle:** Add dataset to notebook, it will be available at `/kaggle/input/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup paths based on environment\n",
    "if not IS_KAGGLE:\n",
    "    # ‚ö†Ô∏è UPDATE THIS PATH TO YOUR ZIP FILE ON GOOGLE DRIVE\n",
    "    ZIP_PATH = '/content/drive/MyDrive/UIT_HWDB_word.zip'\n",
    "    EXTRACT_DIR = '/content/UIT_HWDB_word'\n",
    "    \n",
    "    print(f\"üì¶ Colab: Extracting dataset from: {ZIP_PATH}\")\n",
    "    print(f\"üìÇ Extracting to: {EXTRACT_DIR}\")\n",
    "    \n",
    "    # Extract zip file\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    \n",
    "    print(f\"‚úÖ Extraction complete!\")\n",
    "    \n",
    "else:\n",
    "    # Kaggle settings\n",
    "    # ‚ö†Ô∏è UPDATE 'your-dataset-name' to match your Kaggle dataset\n",
    "    KAGGLE_INPUT = '/kaggle/input/uit-hwdb-word/UIT_HWDB_word'  # Change this to your dataset name\n",
    "    \n",
    "    # Check if zip file exists in Kaggle input\n",
    "    if os.path.exists(KAGGLE_INPUT):\n",
    "        zip_files = [f for f in os.listdir(KAGGLE_INPUT) if f.endswith('.zip')]\n",
    "        \n",
    "        if zip_files:\n",
    "            # Extract from zip\n",
    "            ZIP_PATH = os.path.join(KAGGLE_INPUT, zip_files[0])\n",
    "            EXTRACT_DIR = '/kaggle/working/UIT_HWDB_word'\n",
    "            \n",
    "            print(f\"üì¶ Kaggle: Extracting dataset from: {ZIP_PATH}\")\n",
    "            print(f\"üìÇ Extracting to: {EXTRACT_DIR}\")\n",
    "            \n",
    "            with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/kaggle/working/')\n",
    "            \n",
    "            print(f\"‚úÖ Extraction complete!\")\n",
    "        else:\n",
    "            # Assume dataset is already extracted in Kaggle input\n",
    "            EXTRACT_DIR = KAGGLE_INPUT\n",
    "            print(f\"üìÇ Kaggle: Using dataset from: {EXTRACT_DIR}\")\n",
    "    else:\n",
    "        # Fallback to local path\n",
    "        EXTRACT_DIR = '../data/UIT_HWDB_word'\n",
    "        print(f\"üìÇ Local: Using dataset from: {EXTRACT_DIR}\")\n",
    "\n",
    "# Set test directory\n",
    "TEST_DIR = os.path.join(EXTRACT_DIR, 'test_data')\n",
    "\n",
    "print(f\"\\n‚úì Test directory: {TEST_DIR}\")\n",
    "print(f\"‚úì Test exists: {os.path.exists(TEST_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5ecf4",
   "metadata": {},
   "source": [
    "## 5. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76960185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(base_dir):\n",
    "    \"\"\"\n",
    "    Load test dataset with image paths and ground truth labels.\n",
    "    Returns list of tuples: (image_path, ground_truth_text)\n",
    "    \"\"\"\n",
    "    test_samples = []\n",
    "    folders = sorted(\n",
    "        [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))],\n",
    "        key=lambda x: int(x)\n",
    "    )\n",
    "\n",
    "    for folder in tqdm(folders, desc=f\"Loading {os.path.basename(base_dir)}\"):\n",
    "        label_path = os.path.join(base_dir, folder, 'label.json')\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        # Load labels\n",
    "        with open(label_path, 'r', encoding='utf-8') as f:\n",
    "            labels = json.load(f)\n",
    "\n",
    "        # Process each image\n",
    "        for img_name, label in labels.items():\n",
    "            img_path = os.path.join(base_dir, folder, img_name)\n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "\n",
    "            test_samples.append({\n",
    "                'image_path': img_path,\n",
    "                'ground_truth': label.strip()\n",
    "            })\n",
    "\n",
    "    return test_samples\n",
    "\n",
    "# Load test dataset\n",
    "print(\"\\nüì• Loading test data...\")\n",
    "test_dataset = load_test_dataset(TEST_DIR)\n",
    "print(f\"‚úÖ Loaded {len(test_dataset):,} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8251e9",
   "metadata": {},
   "source": [
    "## 6. Define CER Metric\n",
    "\n",
    "Character Error Rate (CER) measures the edit distance between predicted and ground truth text at character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2839d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import cer\n",
    "\n",
    "def calculate_cer(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate Character Error Rate (CER) for a list of predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predicted texts\n",
    "        references: List of ground truth texts\n",
    "    \n",
    "    Returns:\n",
    "        float: Average CER score (0-1, lower is better)\n",
    "    \"\"\"\n",
    "    return cer(references, predictions)\n",
    "\n",
    "# Test the metric\n",
    "test_pred = \"xin ch√†o\"\n",
    "test_ref = \"xin chao\"\n",
    "print(f\"Example CER: {calculate_cer([test_pred], [test_ref]):.4f}\")\n",
    "print(f\"Perfect match CER: {calculate_cer(['test'], ['test']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d4960",
   "metadata": {},
   "source": [
    "## 7. Run Inference on Test Set\n",
    "\n",
    "This will run inference on all test samples. You can limit the number for faster testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29378716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_inference(model, tokenizer, test_dataset, max_samples=None):\n",
    "    \"\"\"\n",
    "    Run inference on test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded model\n",
    "        tokenizer: Model tokenizer\n",
    "        test_dataset: List of test samples\n",
    "        max_samples: Maximum number of samples to evaluate (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: List of predicted texts\n",
    "        ground_truths: List of ground truth texts\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    # Limit samples if specified\n",
    "    samples_to_process = test_dataset[:max_samples] if max_samples else test_dataset\n",
    "    \n",
    "    prompt = \"<image>\\nFree OCR. \"\n",
    "    \n",
    "    for sample in tqdm(samples_to_process, desc=\"Running inference\"):\n",
    "        try:\n",
    "            # Run inference\n",
    "            result = model.infer(\n",
    "                tokenizer,\n",
    "                prompt=prompt,\n",
    "                image_file=sample['image_path'],\n",
    "                output_path='./output',\n",
    "                base_size=1024,\n",
    "                image_size=640,\n",
    "                crop_mode=True,\n",
    "                save_results=False,\n",
    "                test_compress=False\n",
    "            )\n",
    "            \n",
    "            predictions.append(result.strip())\n",
    "            ground_truths.append(sample['ground_truth'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error processing {sample['image_path']}: {e}\")\n",
    "            # Add empty prediction to maintain alignment\n",
    "            predictions.append(\"\")\n",
    "            ground_truths.append(sample['ground_truth'])\n",
    "    \n",
    "    return predictions, ground_truths\n",
    "\n",
    "# Run inference on test set\n",
    "# Option 1: Run on full test set (takes longer)\n",
    "# predictions, ground_truths = run_inference(model, tokenizer, test_dataset)\n",
    "\n",
    "# Option 2: Run on subset for faster testing (uncomment below)\n",
    "print(\"\\nüîÑ Running inference on test set...\")\n",
    "predictions, ground_truths = run_inference(model, tokenizer, test_dataset, max_samples=500)\n",
    "\n",
    "print(f\"\\n‚úÖ Completed inference on {len(predictions):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1bf5f",
   "metadata": {},
   "source": [
    "## 8. Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CER\n",
    "overall_cer = calculate_cer(predictions, ground_truths)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"üìä EVALUATION RESULTS - {MODEL_TYPE.upper()} MODEL\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìà Character Error Rate (CER): {overall_cer:.4f} ({overall_cer*100:.2f}%)\")\n",
    "print(f\"üìä Total samples evaluated: {len(predictions):,}\")\n",
    "print(f\"‚úì Accuracy (1 - CER): {(1-overall_cer)*100:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e3a19",
   "metadata": {},
   "source": [
    "## 9. Show Sample Predictions\n",
    "\n",
    "Display some example predictions to qualitatively assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "# Show random samples\n",
    "num_samples = 10\n",
    "sample_indices = random.sample(range(len(predictions)), min(num_samples, len(predictions)))\n",
    "\n",
    "print(\"\\nüîç Sample Predictions:\\n\")\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Ground Truth: '{ground_truths[idx]}'\")\n",
    "    print(f\"Prediction:   '{predictions[idx]}'\")\n",
    "    \n",
    "    # Calculate CER for this sample\n",
    "    sample_cer = calculate_cer([predictions[idx]], [ground_truths[idx]])\n",
    "    match = \"‚úÖ\" if predictions[idx] == ground_truths[idx] else \"‚ùå\"\n",
    "    print(f\"CER: {sample_cer:.4f} {match}\")\n",
    "    \n",
    "    # Display image\n",
    "    img_path = test_dataset[idx]['image_path']\n",
    "    img = Image.open(img_path)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d9e37",
   "metadata": {},
   "source": [
    "## 10. Detailed Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-sample CER\n",
    "sample_cers = []\n",
    "exact_matches = 0\n",
    "\n",
    "for pred, ref in zip(predictions, ground_truths):\n",
    "    sample_cer = calculate_cer([pred], [ref])\n",
    "    sample_cers.append(sample_cer)\n",
    "    if pred == ref:\n",
    "        exact_matches += 1\n",
    "\n",
    "# Statistics\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nüìä DETAILED ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n‚úì Exact matches: {exact_matches}/{len(predictions)} ({exact_matches/len(predictions)*100:.2f}%)\")\n",
    "print(f\"\\nüìà CER Statistics:\")\n",
    "print(f\"  - Mean CER: {np.mean(sample_cers):.4f}\")\n",
    "print(f\"  - Median CER: {np.median(sample_cers):.4f}\")\n",
    "print(f\"  - Min CER: {np.min(sample_cers):.4f}\")\n",
    "print(f\"  - Max CER: {np.max(sample_cers):.4f}\")\n",
    "print(f\"  - Std Dev: {np.std(sample_cers):.4f}\")\n",
    "\n",
    "# CER distribution\n",
    "print(f\"\\nüìä CER Distribution:\")\n",
    "perfect = sum(1 for c in sample_cers if c == 0.0)\n",
    "low = sum(1 for c in sample_cers if 0.0 < c <= 0.1)\n",
    "medium = sum(1 for c in sample_cers if 0.1 < c <= 0.3)\n",
    "high = sum(1 for c in sample_cers if c > 0.3)\n",
    "\n",
    "print(f\"  - Perfect (CER = 0.0): {perfect} ({perfect/len(sample_cers)*100:.2f}%)\")\n",
    "print(f\"  - Low (0.0 < CER ‚â§ 0.1): {low} ({low/len(sample_cers)*100:.2f}%)\")\n",
    "print(f\"  - Medium (0.1 < CER ‚â§ 0.3): {medium} ({medium/len(sample_cers)*100:.2f}%)\")\n",
    "print(f\"  - High (CER > 0.3): {high} ({high/len(sample_cers)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc28694",
   "metadata": {},
   "source": [
    "## 11. Show Worst Predictions\n",
    "\n",
    "Identify and display samples with highest CER for error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5acefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find worst predictions\n",
    "sorted_indices = np.argsort(sample_cers)[::-1]\n",
    "worst_n = 5\n",
    "\n",
    "print(f\"\\n‚ùå Top {worst_n} Worst Predictions:\\n\")\n",
    "for i, idx in enumerate(sorted_indices[:worst_n], 1):\n",
    "    print(f\"\\n--- Worst #{i} (CER: {sample_cers[idx]:.4f}) ---\")\n",
    "    print(f\"Ground Truth: '{ground_truths[idx]}'\")\n",
    "    print(f\"Prediction:   '{predictions[idx]}'\")\n",
    "    \n",
    "    # Display image\n",
    "    img_path = test_dataset[idx]['image_path']\n",
    "    img = Image.open(img_path)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec70395",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save evaluation results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61df929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'image_path': [test_dataset[i]['image_path'] for i in range(len(predictions))],\n",
    "    'ground_truth': ground_truths,\n",
    "    'prediction': predictions,\n",
    "    'cer': sample_cers\n",
    "})\n",
    "\n",
    "# Setup save path based on environment\n",
    "if not IS_KAGGLE:\n",
    "    RESULTS_PATH = f\"/content/drive/MyDrive/evaluation_results_{MODEL_TYPE}.csv\"\n",
    "    print(f\"üíæ Colab: Saving results to Google Drive...\")\n",
    "else:\n",
    "    RESULTS_PATH = f\"/kaggle/working/evaluation_results_{MODEL_TYPE}.csv\"\n",
    "    print(f\"üíæ Kaggle: Saving results to working directory...\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(RESULTS_PATH, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Results saved to: {RESULTS_PATH}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìã Results Summary:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'model_type': MODEL_TYPE,\n",
    "    'overall_cer': overall_cer,\n",
    "    'total_samples': len(predictions),\n",
    "    'exact_matches': exact_matches,\n",
    "    'accuracy': 1 - overall_cer,\n",
    "    'mean_cer': np.mean(sample_cers),\n",
    "    'median_cer': np.median(sample_cers),\n",
    "    'std_cer': np.std(sample_cers)\n",
    "}\n",
    "\n",
    "summary_path = RESULTS_PATH.replace('.csv', '_summary.json')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff13ea8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Done! üéâ\n",
    "\n",
    "You've successfully evaluated the model on Vietnamese handwriting test data.\n",
    "\n",
    "**Summary:**\n",
    "- ‚úÖ Selected model type (baseline or finetuned)\n",
    "- ‚úÖ Loaded model and ran inference\n",
    "- ‚úÖ Calculated Character Error Rate (CER)\n",
    "- ‚úÖ Performed error analysis\n",
    "- ‚úÖ Saved detailed results\n",
    "\n",
    "**Next steps:**\n",
    "- Change `MODEL_TYPE` to evaluate the other model\n",
    "- Compare baseline vs finetuned performance\n",
    "- Try different inference parameters (base_size, crop_mode)\n",
    "- Evaluate on full test set"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
